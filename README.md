# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏î‡πâ‡∏ß‡∏¢ OpenSearch ‡πÅ‡∏•‡∏∞ LLM

## ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö

```mermaid
graph TD
    subgraph "OpenSearch"
        A[Vector Index] --- B[Hybrid Search Pipeline]
        B --- C[Document Storage]
        C --- D[Embedding Engine]
    end
    
    subgraph "API Server"
        E[FastAPI Endpoints] --- F[Query Processor]
        F --- G[Vector Client]
        G --- H[Embedding Model]
    end
    
    subgraph "LLM (Ollama)"
        I[Qwen2.5 7B/13B] --- J[Generation API]
        J --- K[Context Processing]
    end
    
    L[Streamlit Frontend]
    
    A --> G
    B --> F
    D --> H
    E --> L
    F --> J
    K --> L
    
    classDef opensearch fill:#e6f3ff,stroke:#0073cf,stroke-width:1px
    classDef api fill:#ebf9e6,stroke:#37a806,stroke-width:1px
    classDef llm fill:#f8eaff,stroke:#a020f0,stroke-width:1px
    classDef frontend fill:#fff,stroke:#ff6347,stroke-width:1px
    
    class A,B,C,D opensearch
    class E,F,G,H api
    class I,J,K llm
    class L frontend
```

‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ 3 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å: OpenSearch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, API Server ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ‡πÅ‡∏•‡∏∞ LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Streamlit Frontend ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ

‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Retrieval-Augmented Generation (RAG) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ OpenSearch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞ LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á e2-standard-4 (4 vCPUs, 16 GB Memory) ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô Ubuntu 25.04

## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

- [‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏î‡πâ‡∏ß‡∏¢ OpenSearch ‡πÅ‡∏•‡∏∞ LLM](#‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö-rag-‡∏î‡πâ‡∏ß‡∏¢-opensearch-‡πÅ‡∏•‡∏∞-llm)
  - [‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö](#‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö)
  - [‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç](#‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç)
  - [‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö](#‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
  - [‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á OpenSearch](#‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á-opensearch)
  - [‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Miniconda ‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°](#‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á-miniconda-‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°)
  - [‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hybrid Search Pipeline](#‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤-hybrid-search-pipeline)
  - [‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Embedding](#‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á-embedding)
  - [‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô API Server](#‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô-api-server)
  - [‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô Frontend](#‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô-frontend)
  - [‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤](#‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤)
    - [‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö OpenSearch](#‡∏õ‡∏±‡∏ç‡∏´‡∏≤-‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö-opensearch)
    - [‡∏õ‡∏±‡∏ç‡∏´‡∏≤: API ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô](#‡∏õ‡∏±‡∏ç‡∏´‡∏≤-api-‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
    - [‡∏õ‡∏±‡∏ç‡∏´‡∏≤: LLM ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á e2-standard-4)](#‡∏õ‡∏±‡∏ç‡∏´‡∏≤-llm-‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô-‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á-e2-standard-4)

## ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö

- ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£: Ubuntu 25.04
- ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥: 16 GB (e2-standard-4)
- vCPU: 4 cores
- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: 20GB ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ
- Docker ‡πÅ‡∏•‡∏∞ Docker Compose

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: ‡∏î‡πâ‡∏ß‡∏¢ 16 GB ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ñ‡∏∂‡∏á‡∏Å‡∏•‡∏≤‡∏á‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô 7B ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ö‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• 13B ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà 32B-70B ‡πÑ‡∏î‡πâ

## ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á OpenSearch

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenSearch:
   ```bash
   docker network create opensearch-net
   ```

2. ‡∏£‡∏±‡∏ô OpenSearch ‡∏î‡πâ‡∏ß‡∏¢ Docker (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á e2-standard-4):
   ```bash
   docker run -e OPENSEARCH_JAVA_OPTS="-Xms2g -Xmx2g" \
     -e discovery.type="single-node" \
     -e DISABLE_SECURITY_PLUGIN="true" \
     -e bootstrap.memory_lock="true" \
     -e cluster.name="opensearch-cluster" \
     -e node.name="os01" \
     -e DISABLE_INSTALL_DEMO_CONFIG="true" \
     --ulimit nofile="65536:65536" \
     --ulimit memlock="-1:-1" \
     --net opensearch-net \
     --restart=always \
     -v opensearch-data:/usr/share/opensearch/data \
     -p 9200:9200 \
     --name=opensearch-single-node \
     opensearchproject/opensearch:latest
   ```
    2.1 ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    ```bash
    docker run -e OPENSEARCH_JAVA_OPTS="-Xms2g -Xmx2g" -e discovery.type="single-node" -e DISABLE_SECURITY_PLUGIN="true" -e bootstrap.memory_lock="true" -e cluster.name="opensearch-cluster" -e node.name="os01" -e DISABLE_INSTALL_DEMO_CONFIG="true" --ulimit nofile="65536:65536" --ulimit memlock="-1:-1" --net opensearch-net --restart=always -v opensearch-data:/usr/share/opensearch/data -p 9200:9200 --name=opensearch-single-node opensearchproject/opensearch:latest
    ```

3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ OpenSearch ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:
   ```bash
   curl -X GET "http://localhost:9200/_cluster/health"
   ```

3.1 ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
```json
{
  "cluster_name": "opensearch-cluster",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 1,
  "number_of_data_nodes": 1,
  "discovered_master": true,
  "discovered_cluster_manager": true,
  "active_primary_shards": 13,
  "active_shards": 13,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 7,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 65
}
```

    3.1.1 ‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡∏∑‡πà‡∏≠ opensearch-cluster ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà
    3.1.2 ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ "status":"yellow" ‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    3.1.3 ‡∏°‡∏µ node ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà 1 node ("number_of_nodes":1)
    3.1.4 ‡∏°‡∏µ shard ‡∏ó‡∏µ‡πà active ‡∏≠‡∏¢‡∏π‡πà 13 shard ("active_shards":13)
    3.1.5 ‡∏°‡∏µ unassigned shards 7 shard ("unassigned_shards":7) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô yellow

‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ "yellow" ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏Å‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á OpenSearch ‡πÅ‡∏ö‡∏ö single node ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å OpenSearch ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ replica shards ‡πÅ‡∏ï‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á node ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß replica shards ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏î‡πâ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ replica ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô node ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å primary)
‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ OpenSearch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ yellow ‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô green ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:

‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô replica ‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö index ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏£‡πâ‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠
‡πÄ‡∏û‡∏¥‡πà‡∏° node ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö production)


## ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Miniconda ‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°

1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Miniconda:
   ```bash
   mkdir -p ~/miniconda3
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
   bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
   ~/miniconda3/bin/conda init bash
   source ~/.bashrc
   ```

2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:
   ```bash
   conda create -n advrag python=3.10
   conda activate advrag
   ```

3. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏û‡πá‡∏Ñ‡πÄ‡∏Å‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:
   ```bash
   pip install llama-index-core llama-index-vector-stores-opensearch llama-index-embeddings-huggingface
   pip install torch torchvision torchaudio
   pip install transformers nest-asyncio fastapi uvicorn streamlit requests
   ```

## ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hybrid Search Pipeline

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Search Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Hybrid Search:
   ```bash
   curl -X PUT "http://localhost:9200/_search/pipeline/hybrid-search-pipeline" \
     -H "Content-Type: application/json" \
     -d '{
     "description": "Post processor for hybrid search",
     "phase_results_processors": [
       {
         "normalization-processor": {
           "normalization": {
             "technique": "min_max"
           },
           "combination": {
             "technique": "arithmetic_mean",
             "parameters": {
               "weights": [
                 0.3,
                 0.7
               ]
             }
           }
         }
       }
     ]
   }'
   ```

    1.1 ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß:
    ```bash
    curl -X PUT "http://localhost:9200/_search/pipeline/hybrid-search-pipeline" -H "Content-Type: application/json" -d "{\"description\": \"Post processor for hybrid search\", \"phase_results_processors\": [{\"normalization-processor\": {\"normalization\": {\"technique\": \"min_max\"}, \"combination\": {\"technique\": \"arithmetic_mean\", \"parameters\": {\"weights\": [0.3, 0.7]}}}}]}"
    ```

2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ pipeline ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß:
   ```bash
   curl -X GET "http://localhost:9200/_search/pipeline/hybrid-search-pipeline"
   ```

## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Embedding

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `embedding.py` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings:
   ```python
   import os
   import torch
   import nest_asyncio
   import time
   import random
   from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext
   from llama_index.core.node_parser import MarkdownNodeParser
   from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient
   from llama_index.embeddings.huggingface import HuggingFaceEmbedding
   import pickle

   # Apply nest_asyncio
   nest_asyncio.apply()

   # Check device
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   print(f"Using device: {device}")

   # Setup OpenSearch
   endpoint = os.getenv("OPENSEARCH_ENDPOINT", "http://localhost:9200")
   idx = os.getenv("OPENSEARCH_INDEX", "dg_md_index")
   text_field = "content"
   embedding_field = "embedding"

   # Load documents
   reader = SimpleDirectoryReader(
       input_dir="md_corpus", 
       recursive=True,
       required_exts=[".md", ".markdown"]
   )
   documents = reader.load_data()
   print(f"Loaded {len(documents)} markdown documents")

   # Create nodes
   md_parser = MarkdownNodeParser(chunk_size=1024)
   nodes = md_parser.get_nodes_from_documents(documents, show_progress=True)
   print(f"Created {len(nodes)} nodes")

   # Setup embedding model
   embedding_model_name = 'BAAI/bge-m3'
   embed_model = HuggingFaceEmbedding(model_name=embedding_model_name, max_length=1024, device=device)

   # Get embedding dimension
   embeddings = embed_model.get_text_embedding("box")
   dim = len(embeddings)
   print(f"Embedding dimension: {dim}")

   # Setup OpenSearch client with retry
   def connect_with_retry(max_retries=5, initial_backoff=1, max_backoff=60):
       retries = 0
       while retries < max_retries:
           try:
               client = OpensearchVectorClient(
                   endpoint=endpoint,
                   index=idx,
                   dim=dim,
                   embedding_field=embedding_field,
                   text_field=text_field,
                   engine="faiss"
               )
               return client
           except Exception as e:
               retries += 1
               print(f"Error: {str(e)}")
               if retries == max_retries:
                   raise e
               
               backoff = min(initial_backoff * (2 ** (retries - 1)), max_backoff)
               jitter = random.uniform(0, 0.1 * backoff)
               sleep_time = backoff + jitter
               
               print(f"Connection failed. Retrying in {sleep_time:.2f} seconds... (Attempt {retries}/{max_retries})")
               time.sleep(sleep_time)

   # Connect to OpenSearch
   client = connect_with_retry()
   vector_store = OpensearchVectorStore(client)
   storage_context = StorageContext.from_defaults(vector_store=vector_store)

   # Create index
   index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model=embed_model)

   # Save index
   with open('md_index.pkl', 'wb') as f:
       pickle.dump(index, f)
   print("Index created and saved to md_index.pkl")
   ```

2. ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings:
   ```bash
   python embedding.py
   ```

## ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô API Server

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `api.py` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô API server:
   ```python
   from fastapi import FastAPI, HTTPException
   from pydantic import BaseModel
   from typing import List, Optional
   from llama_index.vector_stores.opensearch import OpensearchVectorClient, OpensearchVectorStore
   from llama_index.embeddings.huggingface import HuggingFaceEmbedding
   from llama_index.core import VectorStoreIndex, StorageContext
   from llama_index.core.vector_stores.types import VectorStoreQueryMode
   import torch
   from os import getenv
   import nest_asyncio
   import asyncio
   from transformers import AutoTokenizer
   import re

   # Apply nest_asyncio
   nest_asyncio.apply()

   app = FastAPI()

   # Define input data model
   class QueryRequest(BaseModel):
       query: str

   # Initialize HuggingFace embedding model
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   embedding_model_name = 'BAAI/bge-m3'
   embed_model = HuggingFaceEmbedding(model_name=embedding_model_name, max_length=1024, device=device)

   # Get embedding dimension
   embeddings = embed_model.get_text_embedding("test")
   dim = len(embeddings)

   # Initialize OpenSearch client and vector store
   endpoint = getenv("OPENSEARCH_ENDPOINT", "http://localhost:9200")
   idx = getenv("OPENSEARCH_INDEX", "dg_md_index")
   client = OpensearchVectorClient(
       endpoint=endpoint,
       index=idx,
       dim=dim,
       embedding_field="embedding",
       text_field="content",
       engine="faiss"
   )
   vector_store = OpensearchVectorStore(client)

   # Initialize storage context and vector store index
   storage_context = StorageContext.from_defaults(vector_store=vector_store)
   index = VectorStoreIndex(
       nodes=[], storage_context=storage_context, embed_model=embed_model
   )
   retriever = index.as_retriever(similarity_top_k=3, vector_store_query_mode=VectorStoreQueryMode.DEFAULT)

   # Initialize tokenizer for counting tokens
   tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)

   def count_tokens(text: str) -> int:
       return len(tokenizer.encode(text))

   def extract_section_from_metadata(metadata: dict) -> str:
       """Extract section name from metadata based on markdown headers"""
       if "header_text" in metadata:
           return metadata["header_text"]
       
       # Try to find parent header if available
       if "headers" in metadata and metadata["headers"]:
           headers = metadata["headers"]
           if isinstance(headers, list) and len(headers) > 0:
               return headers[-1]  # Get the most specific header
       
       # If no header info is available, try to extract from content
       if "section" in metadata:
           return metadata["section"]
       
       # Fallback to document title or filename if available
       if "title" in metadata:
           return metadata["title"]
       
       # Final fallback
       if "file_path" in metadata:
           filename = metadata["file_path"].split("/")[-1]
           return f"File: {filename}"
       
       return "Unknown Section"

   async def retrieve_query(query: str):
       results = retriever.retrieve(query)
       expanded_results = []
       
       for r in results:
           # Get full content of the node
           try:
               expanded_text = r.node.get_content()
           except AttributeError:
               # If get_content() doesn't exist, try text property directly
               expanded_text = getattr(r, "text", str(r))
           
           # Clean up the text by removing excessive newlines or spaces
           expanded_text = re.sub(r'\n{3,}', '\n\n', expanded_text)
           expanded_text = re.sub(r'\s{2,}', ' ', expanded_text)
           
           # Truncate text to fit within token limit if needed
           token_count = count_tokens(expanded_text)
           if token_count > 512:
               # Simple approach: truncate to first 512 tokens
               words = expanded_text.split()
               truncated_text = ""
               current_tokens = 0
               
               for word in words:
                   word_tokens = count_tokens(word + " ")
                   if current_tokens + word_tokens > 500:  # Leave some margin
                       truncated_text += "..."
                       break
                   truncated_text += word + " "
                   current_tokens += word_tokens
                   
               expanded_text = truncated_text
           
           expanded_results.append((r, expanded_text))
       
       return expanded_results

   @app.post("/search")
   async def search(request: QueryRequest):
       try:
           results = await asyncio.create_task(retrieve_query(request.query))
           response = []
           total_tokens = 0
           
           for r, expanded_text in results:
               tokens = count_tokens(expanded_text)
               total_tokens += tokens
               
               # Extract section information from metadata
               section = extract_section_from_metadata(r.metadata)
               
               response.append({
                   "metadata": r.metadata,
                   "text": expanded_text,
                   "file_path": r.metadata.get("file_path", "N/A"),
                   "tokens": tokens,
                   "page_label": section  # Use section instead of page_label for markdown
               })
               
           return {"results": response, "total_tokens": total_tokens}
       except Exception as e:
           raise HTTPException(status_code=500, detail=str(e))

   if __name__ == "__main__":
       import uvicorn
       uvicorn.run(app, host="0.0.0.0", port=9000)
   ```

2. ‡∏£‡∏±‡∏ô API server:
   ```bash
   python api.py
   ```

3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ API ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:
   ```bash
   curl -X POST "http://localhost:9000/search" -H "Content-Type: application/json" -d '{"query":"‡πÇ‡∏£‡∏Ñ‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠"}'
   ```

## ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô Frontend

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `app.py` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô frontend:
   ```python
   import streamlit as st
   import requests
   import re
   import os
   import base64
   import markdown
   from bs4 import BeautifulSoup

   # Set page config
   st.set_page_config(
       page_title="‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
       page_icon="üîç",
       layout="wide"
   )

   # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á e2-standard-4 ‡∏ó‡∏µ‡πà‡∏°‡∏µ memory 16GB ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• 7B ‡∏´‡∏£‡∏∑‡∏≠ 13B ‡πÑ‡∏î‡πâ

   # Define your system prompt
   system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û
   ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
   1. ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÉ‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
   2. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ"
   3. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
   4. ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
   5. ‡∏™‡∏£‡∏∏‡∏õ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏≤‡∏°‡πÑ‡∏ß‡πâ‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"""

   # Initialize session state
   if "chat_history" not in st.session_state:
       st.session_state.chat_history = []

   if "llm_prompt" not in st.session_state:
       st.session_state.llm_prompt = ""

   # Helper functions
   def add_to_chat_history(entry_type, content):
       st.session_state.chat_history.append((entry_type, content))

   def process_question(question):
       add_to_chat_history("User", question)

       # Call API for search
       try:
           response = requests.post("http://localhost:9000/search", json={"query": question})
           if response.status_code == 200:
               data = response.json()
               search_results = data["results"]
               total_tokens = data["total_tokens"]
               
               # Format results
               response_text = "\n\n".join([
                   f"Text: {result['text']}\nFile Path: {result['file_path']}\nTokens: {result['tokens']}\nSection: {result['page_label']}" 
                   for result in search_results
               ])

               # Create LLM prompt
               llm_prompt = f"""<s>[INST] <<SYS>>
               {system_prompt}
               <</SYS>>

               ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

               ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:
               {response_text}

               ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
               [/INST]"""

               st.session_state.llm_prompt = llm_prompt

               # Call LLM
               try:
                   llm_payload = {
                       "model": "qwen2.5:7b",  # ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ 16GB memory
                       "stream": False,
                       "prompt": llm_prompt,
                   }
                   llm_response = requests.post("http://localhost:11434/api/generate", json=llm_payload)
                   
                   if llm_response.status_code == 200:
                       llm_output = llm_response.json()["response"]
                       add_to_chat_history("Search Results", response_text)
                       add_to_chat_history("AI", llm_output)
                       add_to_chat_history("Total Tokens", str(total_tokens))
                   else:
                       add_to_chat_history("Error", f"LLM Error: {llm_response.status_code}")
                       add_to_chat_history("Search Results", response_text)
               except Exception as e:
                   add_to_chat_history("Error", f"LLM Error: {str(e)}")
                   add_to_chat_history("Search Results", response_text)
           else:
               add_to_chat_history("Error", f"Search Error: {response.status_code}")
       except Exception as e:
           add_to_chat_history("Error", f"Search Error: {str(e)}")

   # Main UI
   st.title("‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û")
   st.write("‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û")

   # Input form
   with st.form(key='question_form'):
       user_input = st.text_input("‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:", key="input")
       submit_button = st.form_submit_button(label='‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°')

   if submit_button and user_input:
       process_question(user_input)

   # Display chat history
   if st.session_state.chat_history:
       for i in range(0, len(st.session_state.chat_history), 4):
           interaction = st.session_state.chat_history[i:i+4]
           
           # Display question
           user_question = next((content for entry_type, content in interaction if entry_type == "User"), None)
           if user_question:
               st.markdown("### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
               st.markdown(f"""<div style="background-color: #ffeeba; padding: 10px; border-radius: 5px; 
                          font-size: 18px; color: #856404; border: 1px solid #ffeeba; margin-bottom: 10px;">
                          {user_question}</div>""", unsafe_allow_html=True)
           
           # Display search results
           search_results = next((content for entry_type, content in interaction if entry_type == "Search Results"), None)
           if search_results:
               with st.expander("‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", expanded=False):
                   st.text(search_results)
           
           # Display AI response
           ai_response = next((content for entry_type, content in interaction if entry_type == "AI"), None)
           if ai_response:
               st.markdown("### ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
               st.markdown(f"""<div style="background-color: #f8d7da; padding: 10px; border-radius: 5px; 
                          font-size: 18px; color: #721c24;">{ai_response}</div>""", unsafe_allow_html=True)
           
           st.markdown("---")

   # Show prompt
   if st.session_state.llm_prompt:
       with st.expander("‡∏î‡∏π Prompt ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ", expanded=False):
           st.code(st.session_state.llm_prompt, language="text")

   # Clear chat history
   if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"):
       st.session_state.chat_history = []
       st.session_state.llm_prompt = ""
       st.rerun()
   ```

2. ‡∏£‡∏±‡∏ô Streamlit app:
   ```bash
   streamlit run app.py --server.address 0.0.0.0
   ```

3. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Streamlit app ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà http://[‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á-IP]:8501

## ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö OpenSearch

1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ OpenSearch ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:
   ```bash
   docker ps | grep opensearch
   ```

2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡∏Ç‡∏≠‡∏á OpenSearch:
   ```bash
   docker logs opensearch-single-node
   ```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: API ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ API ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:
   ```bash
   curl -X POST "http://localhost:9000/search" -H "Content-Type: application/json" -d '{"query":"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ"}'
   ```

2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå md_index.pkl ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á:
   ```bash
   ls -la md_index.pkl
   ```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: LLM ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á e2-standard-4)

1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ:
   ```bash
   free -h
   ```

2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Ollama:
   ```bash
   curl -X POST "http://localhost:11434/api/generate" \
     -H "Content-Type: application/json" \
     -d '{"model":"qwen2.5:7b","prompt":"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ","stream":false}'
   ```
   
3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ 16GB):
   ```bash
   ollama pull qwen2.5:7b  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 7-8GB memory
   # ‡∏´‡∏£‡∏∑‡∏≠
   ollama pull phi3:mini  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 4-5GB memory
   ```

4. **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ 16GB memory ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 13B ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏Ç‡∏≠‡∏á OpenSearch ‡∏•‡∏á‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
